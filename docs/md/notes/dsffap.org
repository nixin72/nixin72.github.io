* Distributed Systems for Fun and Profit
** Designing Systems
- Lots of assumptions you can make
  - Weak assumptions -> Robust system model
  - Strong assumptions -> Easier system to reason about/design

*** Nodes
 - Can execute code
 - Can store data
 - Have a clock

*** Links
- Connect Nodes
- Bi-directional
- Nodes or network can fail

*** Timing/Ordering
- Physical distance between nodes slows things down
- Messages recieved at different times
- Synchronous vs async
  - Accurate clock                     vs clocks useless
  - Upper bound on delay         vs no bounds on delay
  - Convenient                          vs
  - Makes lots of assumptions  vs few/weak assumptions

*** Consensus Problem
Nodes achieve consensus if they...
- All agree on a value
- Produce at most one value
- All reach a decision
- If all correct nodes propose a value, then all nodes decide that value

*** FLP Impossibility Result
Examines the consensus problem (agreement problem)

Assumptions:
- Nodes can only fail by crashing
- Network is reliable
- Async system model

#+BEGIN_QUOTE
There cannot exist a deterministic algorithm for the consensus problem in an asynchronous system
subject to failures, even if messages can never be lost, at most one process may fail, and it can
only fail by crashing.
#+END_QUOTE

No way to solve the consensus problem under minimal system in a way that cannot be delayed forever.
If it existed, one could devise and execution of it where it remains undecided for arbitrary amount
of time by delaying the message delivery.

*** The CAP Theorem
Useful way to think about tradeoffs in the guarantees that a system design makes
Theorem states that only 2/3 of these properties can be satisfied at the same time
- *Consistency*: all nodes see the same data at the same time
- *Availability*: nodes failures do not prevent survivors from continuing to operate
- *Partition tolerance*: The system continues to operate desipite message loss due to network and/or
  node failure

3 types of resulting systems
- CA (consistency + availability)
- CP (consistency + partition tolerance)
- AP (availability + partition tolerance)

CA and CP both provide *strong consistency*. AP systems can provide *eventual consistency*.

CA systems cannot tolerate any node failures. Does not distinguish between node and network failure.
Only safe thing to do is stop accepting writes, otherwise you introduce divergence.

CP systems can tolerate up to ~f~ faults, given ~2f+1~ nodes. CP systems prevent divergence by forcing
asymmetric behaviour on the two sides of the partition.

CP systems incorporate network partitions into their failure model and distringuish between a
majority and minority partition using an algorithm like paxos, raft or viewstamped replication.
CA systems are not partition-aware, and are more common. They often use the two-phase-commit (2PC)
algo and are common in traditional distributed relational databases.

Assuming that a partition occurs, the theorem reduces to CP and AP.

4 conclusions that can be drawn here:
- Many early systems didn't take partition tolerance into account. It wasn't as important because
  systems are not geographically distributed
- There is a tension between strong consistency and high availability  during network partitions.
- There is a tension between strong consistency and performance in normal operation.
- If we do not want to give up availability during a network partition, then we need to explore
  whether consistency models other than strong consistency are workable for our purpose

#+BEGIN_QUOTE
For example, even if user data is georeplicated to multiple datacenters, and the link between those
two datacenters is temporarily out of order, in many cases we'll still want to allow the user to use
the website / service. This means reconciling two divergent sets of data later on, which is both a
technical challenge and a business risk. But often both the technical challenge and the business
risk are manageable, and so it is preferable to provide high availability.
#+END_QUOTE
I believe this is what Nathan's blog post about beating CAP theorem relates to - using immutability,
we can significantly decrease the challenge of reconciling divergent sets of data.

*** Consistency models

The C in CAP refers to *strong consistency*, but that's not the only type of consistency

***** Strong consistency models
capable of miantaining a single copy
- linearizable consistency
- Seqential consistency

****** Linearizable consistency
All operations appear to have executed atomically in an order that is consistent with the global
real-time ordering of operations

****** Sequential Consistency
All operations appear to have executed atomically in some order that is consistent with the order
seen at individual nodes and that is equal at all nodes

Diff is that linearizable requires the order they take effect to be the same as the actual real-time
ordering of operations. Sequential consistency requires they're the same across all nodes, even if
it differs from the real-time ordering of operations.

They are equivalent from the perspective of a client interacting with a node.

***** Weak consistency models
Not stong
- Client-centric consistency models
- Causal consistency: strongest model available
- Eventual consistency models

Strong consistency models guarantee that the apparent order and visibility of updates is equivalent
to a non-replicated system. Weak consistency models, on the other hand, do not make such guarantees.
^^ Again, I believe this is part of what Nathan's blog post refers to
Hmm, but time-stamps aren't necessarily reliable, so how is this handled...?

****** Client-centric consistency models
Consistency models that involve the nothion of a client or session in some way. Ex, a client-centric
consistency model might guarantee that a client will never see older versions of a data item. This
is often implemented by building additional caching in to the client lib, so that if a client moves
to a replica node that contains old data, then the client lib returns its cached value rather than
the old value from the replica.

Many typed of client-centric consistency models

****** Eventual consistency
If you stop changing values, after some amount of time, all replicas will agree on the same value.

This is a very weak constraint because most useful distributed systems will have eventual
consistency.

** Time and Order

*** 2PC (two phase commit)

assumptions:
- Data in stable storage at each node is never lost
- No node crashes forever

Allows changes to be commited by having each node vote on wheter to commit or abort the transaction.
In P/B systems, there is no step for rolling back an op that has failed on some nodes and succeeded
on others, which causes replicas to potentially diverge.

Prone to blocking since a single node failure blocks until it's recovered.

Data loss is possible when stable storage is corrupted in a crash

(CA)P - failure models do not include network partitions

*** Partition tolerant consensus algorithms

Paxos and Raft algorithms
Paxos is the classic algol, but it's difficult to explain and implement. Raft is much newer and easier.

***** What is a Network Partition?
The failure of a network link to one or several nodes.
The nodes themselves continue to stay active, and as such may be able to receive requests from
clients on their side of the network partition.

Network partitions should also be assumed to occur

Not possible to distinguish between a failed remote node, and that node being unreachable. If a
network partition occurs bu no nodes fail, then the system is divided into two partitions which are
simultaneously active.

** Replication: weak consistency model protocols

*** Reconciling different operation orders

Systems that do not enforce single-copy consistency allow replicas to diverge from each other. This
means that there is not strictly defined pattern of communication: replicas can be separated from
each other and yet continue to be availble and accept writes.

*single-copy consistency:* requires that you need to figure out what to do when nodes go down
*weak consistency*: requires that you need to figure out how to reconcile diverging data

Ex. 3 replicas partitioned from each other accepting writes. They can all see each other again and
need to converge to the same result:

#+BEGIN_SRC
[A] \
    --> [merge]
[B] /     |
          |
[C] ----[merge]---> result
#+END_SRC

There is no single total order, so we need to figure out how to order shit too.
#+BEGIN_SRC
1: { operation: concat('Hello ') }
2: { operation: concat('World') }
3: { operation: concat('!') }

-> Hello World!
-> World!Hello
-> Hollo !World
...
#+END_SRC

We want replicas to converge to the same result.

*** Amazon's Dynamo

Offers weak consistency guarantees, but high availability. It is eventually consistent, highly
available. K/V store.

Cluster has N peer nodes; each node has set of keys it's responsible for storing

When a key is read, there is a read reconciliation phase that attempts to reconcile differences
between replicas before returning the value back to the client.

**** Consistent Hashing

Need to locate which node the data lives on. Requires key-to-node mapping.

Keys are mapped to nodes using consistent hashing technique. Key can be mapped to a set of nodes by
a simple calculation on a client. Hashing is faster than performing remote procedure call.


**** Partial quorums
